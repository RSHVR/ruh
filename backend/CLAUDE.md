# Backend - FastAPI AI Product Safety Server

## Overview

Python FastAPI backend server that powers the Eject product safety analysis system. Uses Anthropic's Claude AI Agent SDK to analyze products for allergens, PFAS compounds, and harmful substances.

**Architecture**: Clean Architecture pattern with clear separation of concerns across API, Domain, and Infrastructure layers.

---

## Function-Level Flow Diagram (All Backend Functions)

### Entry Point

```
ğŸ“„ run.py::main()
  â”œâ”€ Imports: settings from src.infrastructure.config
  â”œâ”€ Calls: uvicorn.run(
  â”‚          "src.api.main:app",
  â”‚          host="0.0.0.0",
  â”‚          port=8000,
  â”‚          reload=True
  â”‚        )
  â””â”€ Returns: void (starts server)
```

### API Layer Functions

#### FastAPI Application Setup

```
ğŸ“„ src/api/main.py::app (FastAPI instance)
  â”œâ”€ Creates: FastAPI(title="Eject API", version="1.0.0")
  â”œâ”€ Adds: CORSMiddleware(allow_origins=settings.allowed_origins, ...)
  â”œâ”€ Includes: health.router (prefix="/api")
  â”œâ”€ Includes: analyze.router (prefix="/api")
  â””â”€ Returns: app instance
```

#### Authentication

```
ğŸ“„ src/api/auth.py::verify_api_key(
      credentials: HTTPAuthorizationCredentials = Security(security)
    ) â†’ None
  â”œâ”€ Reads: settings.api_key
  â”œâ”€ Compares: credentials.credentials == settings.api_key
  â”œâ”€ IF match: Returns None
  â””â”€ IF mismatch: Raises HTTPException(401, "Invalid API key")
```

#### Health Check Endpoint

```
ğŸ“„ src/api/routes/health.py::get_health() â†’ HealthResponse
  â”œâ”€ Gets: current_time = datetime.now(timezone.utc)
  â””â”€ Returns: HealthResponse(
               status="healthy",
               timestamp=current_time.isoformat()
             )
```

#### Product Analysis Endpoint

```
ğŸ“„ src/api/routes/analyze.py::analyze_product(
      request: AnalysisRequest,
      credentials: HTTPAuthorizationCredentials = Depends(verify_api_key)
    ) â†’ AnalysisResponse
  â”‚
  â”œâ”€ Step 1: AUTHENTICATION
  â”‚   â””â”€ Dependency injection calls verify_api_key(credentials)
  â”‚
  â”œâ”€ Step 2: CACHE CHECK
  â”‚   â”œâ”€ Initializes: db = DatabaseService()
  â”‚   â”œâ”€ Generates: url_hash = db.generate_url_hash(request.product_url)
  â”‚   â”œâ”€ Calls: cached = db.get_cached_analysis(url_hash)
  â”‚   â””â”€ IF cached: Returns AnalysisResponse(cached data, cached=True)
  â”‚
  â”œâ”€ Step 3: SCRAPING
  â”‚   â”œâ”€ Initializes: scraper_service = ProductScraperService()
  â”‚   â”œâ”€ Calls: scraped = scraper_service.try_scrape(request.product_url)
  â”‚   â””â”€ Stores: scraped_product OR None
  â”‚
  â”œâ”€ Step 4: AI ANALYSIS (Two Paths)
  â”‚   â”‚
  â”‚   â”œâ”€ Path A: IF scraped AND confidence > 0.3
  â”‚   â”‚   â”œâ”€ Initializes: claude_query = ClaudeQueryService()
  â”‚   â”‚   â”œâ”€ Calls: product_data = claude_query.extract_product_data(
  â”‚   â”‚   â”‚                          scraped.raw_html_product
  â”‚   â”‚   â”‚                        )
  â”‚   â”‚   â”œâ”€ Initializes: claude_agent = ProductSafetyAgent()
  â”‚   â”‚   â””â”€ Calls: analysis_data = claude_agent.analyze_extracted_product(
  â”‚   â”‚                               product_data,
  â”‚   â”‚                               request.product_url
  â”‚   â”‚                             )
  â”‚   â”‚
  â”‚   â””â”€ Path B: IF scraping failed OR low confidence
  â”‚       â”œâ”€ Initializes: claude_agent = ProductSafetyAgent()
  â”‚       â””â”€ Calls: analysis_data = claude_agent.analyze_product(
  â”‚                                   request.product_url
  â”‚                                 )
  â”‚
  â”œâ”€ Step 5: HARM SCORE CALCULATION
  â”‚   â”œâ”€ Calls: harm_score = HarmScoreCalculator.calculate(analysis_data)
  â”‚   â””â”€ Computes: overall_score = 100 - harm_score
  â”‚
  â”œâ”€ Step 6: BUILD RESPONSE
  â”‚   â””â”€ Creates: analysis = ProductAnalysis(
  â”‚                 product_name=...,
  â”‚                 brand=...,
  â”‚                 overall_score=overall_score,
  â”‚                 allergens_detected=...,
  â”‚                 pfas_detected=...,
  â”‚                 other_concerns=...,
  â”‚                 confidence=...
  â”‚               )
  â”‚
  â”œâ”€ Step 7: CACHE STORAGE
  â”‚   â””â”€ Calls: db.store_analysis(url_hash, request.product_url, analysis_response)
  â”‚
  â””â”€ Step 8: RETURN
      â””â”€ Returns: AnalysisResponse(
                    analysis=analysis,
                    alternatives=[],
                    cached=False,
                    url_hash=url_hash
                  )
```

#### Review Insights Endpoint (Implemented but Unused)

```
ğŸ“„ src/api/routes/analyze.py::get_review_insights(
      request: ReviewInsightsRequest,
      credentials: HTTPAuthorizationCredentials = Depends(verify_api_key)
    ) â†’ ReviewInsightsResponse
  â”‚
  â”œâ”€ Initializes: scraper_service = ProductScraperService()
  â”œâ”€ Calls: scraped = scraper_service.try_scrape(request.product_url)
  â”‚
  â”œâ”€ IF scraped:
  â”‚   â”œâ”€ Initializes: claude_query = ClaudeQueryService()
  â”‚   â””â”€ Calls: insights = claude_query.extract_review_insights(
  â”‚                          scraped.raw_html_product
  â”‚                        )
  â”‚
  â”œâ”€ IF not scraped:
  â”‚   â””â”€ Raises: HTTPException(500, "Failed to scrape reviews")
  â”‚
  â””â”€ Returns: ReviewInsightsResponse(insights=insights)
```

### Domain Layer Functions

#### Harm Score Calculation

```
ğŸ“„ src/domain/harm_calculator.py::HarmScoreCalculator.calculate(
      analysis_data: Dict[str, Any]
    ) â†’ int
  â”‚
  â”œâ”€ Initializes: total_score = 0
  â”‚
  â”œâ”€ Step 1: ALLERGEN SCORING
  â”‚   â”œâ”€ Gets: allergens = analysis_data.get('allergens_detected', [])
  â”‚   â””â”€ For each allergen:
  â”‚       â”œâ”€ Reads: severity = allergen.get('severity', 'moderate')
  â”‚       â”œâ”€ Adds: 5 (low), 15 (moderate), or 30 (high) points
  â”‚       â””â”€ Accumulates: total_score += points
  â”‚
  â”œâ”€ Step 2: PFAS SCORING
  â”‚   â”œâ”€ Gets: pfas = analysis_data.get('pfas_detected', [])
  â”‚   â””â”€ For each PFAS:
  â”‚       â””â”€ Adds: 40 points to total_score
  â”‚
  â”œâ”€ Step 3: OTHER CONCERNS SCORING
  â”‚   â”œâ”€ Gets: concerns = analysis_data.get('other_concerns', [])
  â”‚   â””â”€ For each concern:
  â”‚       â”œâ”€ Reads: toxicity = concern.get('toxicity_level', 'low')
  â”‚       â”œâ”€ Adds: 5 (low), 15 (medium), or 25 (high) points
  â”‚       â””â”€ Accumulates: total_score += points
  â”‚
  â”œâ”€ Step 4: CATEGORY MULTIPLIERS
  â”‚   â”œâ”€ Gets: category = analysis_data.get('product_category', '')
  â”‚   â”œâ”€ IF 'pesticide' OR 'cleaner': total_score *= 1.3
  â”‚   â””â”€ IF 'food': total_score *= 1.2
  â”‚
  â”œâ”€ Step 5: CAPPING
  â”‚   â””â”€ Applies: total_score = max(0, min(100, int(total_score)))
  â”‚
  â””â”€ Returns: total_score (0-100)
```

### Infrastructure Layer Functions

#### Configuration

```
ğŸ“„ src/infrastructure/config.py::Settings (Pydantic BaseSettings)
  â”œâ”€ Loads: Environment variables from .env
  â”œâ”€ Defines: anthropic_api_key, supabase_url, supabase_key, api_key, allowed_origins
  â””â”€ Exports: settings = Settings() singleton
```

#### Database Operations

```
ğŸ“„ src/infrastructure/database.py::DatabaseService.__init__()
  â”œâ”€ Reads: settings.supabase_url, settings.supabase_key
  â”œâ”€ Calls: create_client(url, key)
  â””â”€ Stores: self.client = Supabase client

ğŸ“„ src/infrastructure/database.py::generate_url_hash(url: str) â†’ str
  â””â”€ Returns: hashlib.sha256(url.encode()).hexdigest()

ğŸ“„ src/infrastructure/database.py::get_cached_analysis(url_hash: str) â†’ Dict | None
  â”œâ”€ Calls: response = self.client.table('product_analyses')
  â”‚                     .select('*')
  â”‚                     .eq('url_hash', url_hash)
  â”‚                     .execute()
  â”œâ”€ IF response.data: Returns response.data[0]
  â””â”€ ELSE: Returns None

ğŸ“„ src/infrastructure/database.py::store_analysis(
      url_hash: str,
      product_url: str,
      analysis_response: AnalysisResponse
    ) â†’ bool
  â”‚
  â”œâ”€ Builds: db_data = {
  â”‚            'url_hash': url_hash,
  â”‚            'product_url': product_url,
  â”‚            'product_name': analysis.product_name,
  â”‚            'brand': analysis.brand,
  â”‚            'overall_score': analysis.overall_score,
  â”‚            'allergens_detected': json.dumps(allergens),
  â”‚            'pfas_detected': json.dumps(pfas),
  â”‚            'other_concerns': json.dumps(concerns),
  â”‚            'confidence': analysis.confidence,
  â”‚            'analyzed_at': datetime.now(timezone.utc).isoformat()
  â”‚          }
  â”‚
  â”œâ”€ Calls: self.client.table('product_analyses').upsert(db_data).execute()
  â”œâ”€ Returns: True (on success)
  â””â”€ Returns: False (on exception)
```

#### Claude AI Agent

```
ğŸ“„ src/infrastructure/claude_agent.py::ProductSafetyAgent.__init__()
  â”œâ”€ Reads: settings.anthropic_api_key
  â”œâ”€ Creates: self.client = Anthropic(api_key=...)
  â””â”€ Defines: self.tools = [web_search_tool, web_fetch_tool]

ğŸ“„ src/infrastructure/claude_agent.py::analyze_product(url: str) â†’ Dict
  â”‚
  â”œâ”€ Builds: system_prompt = "You are an expert product safety analyst..."
  â”œâ”€ Builds: user_prompt = f"Analyze this product: {url}"
  â”‚
  â”œâ”€ Calls: response = self.client.messages.create(
  â”‚          model="claude-sonnet-4-5-20250929",
  â”‚          max_tokens=8192,
  â”‚          system=system_prompt,
  â”‚          messages=[{role: "user", content: user_prompt}],
  â”‚          tools=[web_search, web_fetch]
  â”‚        )
  â”‚
  â”œâ”€ Enters: Tool use loop
  â”‚   â”œâ”€ WHILE response has stop_reason == "tool_use":
  â”‚   â”‚   â”œâ”€ For each tool_use block:
  â”‚   â”‚   â”‚   â”œâ”€ IF tool == "web_search":
  â”‚   â”‚   â”‚   â”‚   â””â”€ Calls: _execute_web_search(input)
  â”‚   â”‚   â”‚   â””â”€ IF tool == "web_fetch":
  â”‚   â”‚   â”‚       â””â”€ Calls: _execute_web_fetch(url)
  â”‚   â”‚   â”‚
  â”‚   â”‚   â”œâ”€ Builds: tool_results = [{type: "tool_result", ...}]
  â”‚   â”‚   â””â”€ Calls: response = self.client.messages.create(...) with tool_results
  â”‚   â”‚
  â”‚   â””â”€ Continues until stop_reason == "end_turn"
  â”‚
  â”œâ”€ Extracts: final_text = response.content[-1].text
  â”œâ”€ Parses: analysis_data = json.loads(final_text)
  â””â”€ Returns: {allergens_detected, pfas_detected, other_concerns, confidence}

ğŸ“„ src/infrastructure/claude_agent.py::analyze_extracted_product(
      product_data: Dict,
      url: str
    ) â†’ Dict
  â”‚
  â”œâ”€ Similar to analyze_product but:
  â”‚   â”œâ”€ Prompt includes pre-extracted product_data
  â”‚   â”œâ”€ Only web_search tool (no web_fetch needed)
  â”‚   â””â”€ Claude searches for manufacturer safety data
  â”‚
  â””â”€ Returns: analysis_data

ğŸ“„ src/infrastructure/claude_agent.py::_execute_web_search(query: str) â†’ str
  â”œâ”€ Reads: settings.serper_api_key
  â”œâ”€ Calls: httpx.post(
  â”‚          'https://google.serper.dev/search',
  â”‚          json={'q': query},
  â”‚          headers={'X-API-KEY': api_key}
  â”‚        )
  â”œâ”€ Parses: results = response.json()
  â””â”€ Returns: JSON string of search results

ğŸ“„ src/infrastructure/claude_agent.py::_execute_web_fetch(url: str) â†’ str
  â”œâ”€ Calls: httpx.get(url, timeout=10)
  â”œâ”€ Returns: response.text
  â””â”€ Returns: error message (on failure)

ğŸ“„ src/infrastructure/claude_agent.py::find_alternatives(...) â†’ List[Dict]
  â””â”€ Returns: [] (TODO: Phase 4 implementation)
```

#### Claude Query Service (Data Extraction)

```
ğŸ“„ src/infrastructure/claude_query.py::ClaudeQueryService.__init__()
  â”œâ”€ Reads: settings.anthropic_api_key
  â””â”€ Creates: self.client = Anthropic(api_key=...)

ğŸ“„ src/infrastructure/claude_query.py::extract_product_data(html: str) â†’ Dict
  â”‚
  â”œâ”€ Builds: prompt = "Extract structured product data from this HTML..."
  â”œâ”€ Calls: response = self.client.messages.create(
  â”‚          model="claude-sonnet-4-5-20250929",
  â”‚          max_tokens=4096,
  â”‚          messages=[{role: "user", content: prompt + html}]
  â”‚        )
  â”‚   (NO TOOLS - pure extraction)
  â”‚
  â”œâ”€ Extracts: text = response.content[0].text
  â”œâ”€ Parses: data = json.loads(text)
  â””â”€ Returns: {product_name, brand, ingredients, materials, features, ...}

ğŸ“„ src/infrastructure/claude_query.py::extract_review_insights(html: str) â†’ ReviewInsights
  â”‚
  â”œâ”€ Similar to extract_product_data
  â”œâ”€ Prompt: "Extract safety-related review insights..."
  â””â”€ Returns: ReviewInsights(common_concerns, positive_safety_notes, ...)
```

#### Product Scraper Service

```
ğŸ“„ src/infrastructure/product_scraper.py::ProductScraperService.__init__()
  â””â”€ (No initialization needed)

ğŸ“„ src/infrastructure/product_scraper.py::try_scrape(url: str) â†’ ScrapedProduct | None
  â”‚
  â”œâ”€ Calls: scraper = ScraperFactory.get_scraper(url)
  â”‚
  â”œâ”€ IF scraper:
  â”‚   â”œâ”€ Calls: result = await scraper.scrape(url)
  â”‚   â””â”€ Returns: result (ScrapedProduct)
  â”‚
  â””â”€ ELSE:
      â””â”€ Returns: None
```

#### Scraper Factory

```
ğŸ“„ src/infrastructure/scrapers/factory.py::ScraperFactory.get_scraper(
      url: str
    ) â†’ BaseScraper | None
  â”‚
  â”œâ”€ IF 'amazon.com' in url OR 'amazon.' in url:
  â”‚   â””â”€ Returns: AmazonScraper()
  â”‚
  â””â”€ ELSE:
      â””â”€ Returns: None
```

#### Amazon Scraper

```
ğŸ“„ src/infrastructure/scrapers/amazon.py::AmazonScraper.scrape(
      url: str
    ) â†’ ScrapedProduct
  â”‚
  â”œâ”€ Step 1: HTTP FETCH
  â”‚   â”œâ”€ Defines: headers = {'User-Agent': '...', 'Accept-Language': 'en-US', ...}
  â”‚   â”œâ”€ Calls: async with httpx.AsyncClient() as client:
  â”‚   â”‚          response = await client.get(url, headers=headers, timeout=10)
  â”‚   â””â”€ Gets: html = response.text
  â”‚
  â”œâ”€ Step 2: HTML PARSING
  â”‚   â”œâ”€ Creates: soup = BeautifulSoup(html, 'lxml')
  â”‚   â”œâ”€ Extracts: title = soup.select_one('#productTitle').get_text()
  â”‚   â”œâ”€ Extracts: brand = soup.select_one('#bylineInfo').get_text()
  â”‚   â”œâ”€ Extracts: features = soup.select('#feature-bullets li')
  â”‚   â””â”€ Extracts: details = soup.select('#productDetails_detailBullets_sections1')
  â”‚
  â”œâ”€ Step 3: INGREDIENT/MATERIAL EXTRACTION
  â”‚   â”œâ”€ Searches: Keywords like 'ingredients', 'materials', 'composition'
  â”‚   â”œâ”€ Finds: Relevant sections in features and details
  â”‚   â””â”€ Builds: raw_html_product (concatenated relevant sections)
  â”‚
  â”œâ”€ Step 4: CONFIDENCE CALCULATION
  â”‚   â”œâ”€ Initializes: confidence = 0.0
  â”‚   â”œâ”€ IF title found: confidence += 0.3
  â”‚   â”œâ”€ IF brand found: confidence += 0.2
  â”‚   â”œâ”€ IF ingredients found: confidence += 0.5
  â”‚   â””â”€ Caps: confidence = min(1.0, confidence)
  â”‚
  â””â”€ Returns: ScrapedProduct(
                raw_html_product=raw_html_product,
                confidence=confidence
              )
```

### Test Functions

```
ğŸ“„ tests/e2e/test_product_analysis.py::test_analyze_sunscreen()
  â”œâ”€ Creates: client = AsyncClient(transport=ASGITransport(app=app), base_url="http://test")
  â”œâ”€ Calls: response = await client.post('/api/analyze', json={...}, headers={...})
  â”œâ”€ Asserts: response.status_code == 200
  â”œâ”€ Asserts: 'analysis' in response.json()
  â””â”€ Asserts: harm_score is calculated

ğŸ“„ tests/e2e/test_product_analysis.py::test_analyze_lipstick()
  â””â”€ Similar to test_analyze_sunscreen with different product

ğŸ“„ tests/e2e/test_product_analysis.py::test_invalid_api_key()
  â”œâ”€ Calls: POST /api/analyze with wrong API key
  â””â”€ Asserts: response.status_code == 401
```

---

## File-Level Import Relationships

### API Layer Imports

```
src/api/main.py
  IMPORTS:
    - fastapi.{FastAPI, HTTPException}
    - fastapi.middleware.cors.CORSMiddleware
    - src.infrastructure.config.settings
    - src.api.routes.{health, analyze}
  IMPORTED BY:
    - run.py (as module string)
    - tests/e2e/test_product_analysis.py

src/api/auth.py
  IMPORTS:
    - fastapi.{HTTPException, Security}
    - fastapi.security.{HTTPAuthorizationCredentials, HTTPBearer}
    - src.infrastructure.config.settings
  IMPORTED BY:
    - src/api/routes/analyze.py

src/api/routes/health.py
  IMPORTS:
    - fastapi.APIRouter
    - pydantic.BaseModel
    - datetime
  IMPORTED BY:
    - src/api/main.py

src/api/routes/analyze.py
  IMPORTS:
    - fastapi.{APIRouter, HTTPException, Depends}
    - src.domain.models.*
    - src.domain.harm_calculator.HarmScoreCalculator
    - src.infrastructure.claude_agent.ProductSafetyAgent
    - src.infrastructure.product_scraper.ProductScraperService
    - src.infrastructure.claude_query.ClaudeQueryService
    - src.infrastructure.database.DatabaseService
    - src.api.auth.verify_api_key
  IMPORTED BY:
    - src/api/main.py
```

### Domain Layer Imports

```
src/domain/models.py
  IMPORTS:
    - pydantic.{BaseModel, Field}
    - datetime, enum, typing, uuid
  IMPORTED BY:
    - src/api/routes/analyze.py
    - src/infrastructure/claude_query.py
    - src/infrastructure/database.py
    - src/infrastructure/product_scraper.py
    - src/infrastructure/scrapers/amazon.py

src/domain/harm_calculator.py
  IMPORTS:
    - typing.{Dict, Any}
  IMPORTED BY:
    - src/api/routes/analyze.py
```

### Infrastructure Layer Imports

```
src/infrastructure/config.py
  IMPORTS:
    - pydantic_settings.{BaseSettings, SettingsConfigDict}
  IMPORTED BY:
    - run.py
    - src/api/main.py
    - src/api/auth.py
    - src/infrastructure/claude_agent.py
    - src/infrastructure/claude_query.py
    - src/infrastructure/database.py

src/infrastructure/database.py
  IMPORTS:
    - supabase.{create_client, Client}
    - src.infrastructure.config.settings
  IMPORTED BY:
    - src/api/routes/analyze.py

src/infrastructure/claude_agent.py
  IMPORTS:
    - anthropic.{Anthropic, RateLimitError, APIError}
    - src.infrastructure.config.settings
  IMPORTED BY:
    - src/api/routes/analyze.py

src/infrastructure/claude_query.py
  IMPORTS:
    - anthropic.Anthropic
    - src.infrastructure.config.settings
    - src.domain.models.ScrapedProduct
  IMPORTED BY:
    - src/api/routes/analyze.py

src/infrastructure/product_scraper.py
  IMPORTS:
    - src.infrastructure.scrapers.factory.ScraperFactory
    - src.domain.models.ScrapedProduct
  IMPORTED BY:
    - src/api/routes/analyze.py

src/infrastructure/scrapers/factory.py
  IMPORTS:
    - src.infrastructure.scrapers.base.BaseScraper
    - src.infrastructure.scrapers.amazon.AmazonScraper
  IMPORTED BY:
    - src/infrastructure/product_scraper.py

src/infrastructure/scrapers/base.py
  IMPORTS:
    - abc.{ABC, abstractmethod}
  IMPORTED BY:
    - src/infrastructure/scrapers/factory.py
    - src/infrastructure/scrapers/amazon.py

src/infrastructure/scrapers/amazon.py
  IMPORTS:
    - httpx, bs4.BeautifulSoup
    - src.infrastructure.scrapers.base.BaseScraper
    - src.domain.models.ScrapedProduct
  IMPORTED BY:
    - src/infrastructure/scrapers/factory.py
```

---

## Directory Structure

```
/backend/
â”œâ”€â”€ run.py                          # Entry point - starts uvicorn server
â”œâ”€â”€ pyproject.toml                  # Python project config, dependencies
â”œâ”€â”€ requirements.txt                # Pip requirements
â”œâ”€â”€ uv.lock                         # UV lock file
â”œâ”€â”€ Dockerfile                      # Docker container config
â”œâ”€â”€ .env                            # Environment variables (git-ignored)
â”œâ”€â”€ .env.example                    # Example environment config
â”œâ”€â”€ README.md                       # Backend setup docs
â”œâ”€â”€ DEPLOY.md                       # Deployment documentation
â”œâ”€â”€ IMPLEMENTATION_SUMMARY.md       # Implementation summary
â”‚
â”œâ”€â”€ src/                            # Source code â†’ [src/CLAUDE.md](./src/CLAUDE.md)
â”œâ”€â”€ migrations/                     # âš ï¸ BLOAT: Legacy migrations â†’ [migrations/CLAUDE.md](./migrations/CLAUDE.md)
â”œâ”€â”€ supabase/                       # Supabase database â†’ [supabase/CLAUDE.md](./supabase/CLAUDE.md)
â””â”€â”€ tests/                          # Test suite â†’ [tests/CLAUDE.md](./tests/CLAUDE.md)
```

---

## Bloat Identification

### âš ï¸ BLOAT: Legacy Migrations Directory

**Location**: `./migrations/`

**Evidence**:
- Contains old SQL files superseded by `./supabase/migrations/`
- Old schema missing tables and columns
- Not used in production

**Files**:
- `001_create_tables.sql` (outdated schema)
- `002_seed_knowledge_base.sql` (legacy seed data)

---

## Key Technologies

- **Framework**: FastAPI (async Python web framework)
- **AI**: Anthropic Claude Sonnet 4.5 with Agent SDK
- **Database**: Supabase (PostgreSQL with real-time features)
- **Scraping**: httpx + BeautifulSoup4
- **Testing**: pytest with async support
- **Deployment**: Docker + Google Cloud Run

---

## Related Documentation

- [Root Documentation](../CLAUDE.md) - Complete system overview
- [Source Code](./src/CLAUDE.md) - Detailed source code documentation
- [Database](./supabase/CLAUDE.md) - Supabase schema and migrations
- [Tests](./tests/CLAUDE.md) - Test suite documentation

---

Last Updated: 2025-11-18
