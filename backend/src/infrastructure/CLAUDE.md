# Infrastructure Layer - External Services

## Overview

Infrastructure layer handling all external service integrations: Claude AI, Supabase database, web scraping, and configuration management.

---

## Function-Level Flow Diagram (All Infrastructure Functions)

### Configuration Management

```
ðŸ“„ config.py::Settings (Pydantic BaseSettings)
  â”œâ”€ Loads: .env file from environment
  â”œâ”€ Validates: Required fields (anthropic_api_key, supabase_url, etc.)
  â””â”€ Exports: settings singleton

Fields:
  - anthropic_api_key: str
  - supabase_url: str
  - supabase_key: str
  - serper_api_key: str
  - api_key: str
  - allowed_origins: List[str]
```

### Database Service (Supabase)

```
ðŸ“„ database.py::DatabaseService.__init__()
  â”œâ”€ Reads: settings.supabase_url
  â”œâ”€ Reads: settings.supabase_key
  â””â”€ Creates: self.client = create_client(url, key)

ðŸ“„ database.py::generate_url_hash(url: str) â†’ str
  â””â”€ Returns: hashlib.sha256(url.encode()).hexdigest()

ðŸ“„ database.py::get_cached_analysis(url_hash: str) â†’ Dict | None
  â”œâ”€ Calls: response = self.client
  â”‚           .table('product_analyses')
  â”‚           .select('*')
  â”‚           .eq('url_hash', url_hash)
  â”‚           .execute()
  â”œâ”€ IF response.data exists:
  â”‚   â””â”€ Returns: response.data[0]
  â””â”€ ELSE:
      â””â”€ Returns: None

ðŸ“„ database.py::store_analysis(
      url_hash: str,
      product_url: str,
      analysis_response: AnalysisResponse
    ) â†’ bool
  â”œâ”€ Builds: db_data = {
  â”‚            'url_hash': url_hash,
  â”‚            'product_url': product_url,
  â”‚            'product_name': analysis.product_name,
  â”‚            'brand': analysis.brand,
  â”‚            'overall_score': analysis.overall_score,
  â”‚            'allergens_detected': json.dumps(allergens),
  â”‚            'pfas_detected': json.dumps(pfas),
  â”‚            'other_concerns': json.dumps(concerns),
  â”‚            'confidence': analysis.confidence,
  â”‚            'analyzed_at': datetime.now(timezone.utc).isoformat()
  â”‚          }
  â”œâ”€ Calls: self.client.table('product_analyses').upsert(db_data).execute()
  â”œâ”€ Returns: True (on success)
  â””â”€ Returns: False (on exception)
```

### Claude AI Agent (Analysis with Tools)

```
ðŸ“„ claude_agent.py::ProductSafetyAgent.__init__()
  â”œâ”€ Reads: settings.anthropic_api_key
  â”œâ”€ Creates: self.client = Anthropic(api_key=...)
  â””â”€ Defines: self.tools = [web_search_tool, web_fetch_tool]

ðŸ“„ claude_agent.py::analyze_product(url: str) â†’ Dict
  â”‚
  â”œâ”€ Builds: system_prompt = "You are an expert product safety analyst..."
  â”œâ”€ Builds: user_prompt = f"Analyze product at {url} for allergens, PFAS, toxins"
  â”‚
  â”œâ”€ Calls: response = self.client.messages.create(
  â”‚          model="claude-sonnet-4-5-20250929",
  â”‚          max_tokens=8192,
  â”‚          system=system_prompt,
  â”‚          messages=[{role: "user", content: user_prompt}],
  â”‚          tools=[web_search, web_fetch]
  â”‚        )
  â”‚
  â”œâ”€ TOOL USE LOOP:
  â”‚   â”œâ”€ WHILE response.stop_reason == "tool_use":
  â”‚   â”‚   â”œâ”€ For each tool_use in response.content:
  â”‚   â”‚   â”‚   â”œâ”€ IF tool.name == "web_search":
  â”‚   â”‚   â”‚   â”‚   â””â”€ Calls: result = _execute_web_search(tool.input)
  â”‚   â”‚   â”‚   â””â”€ IF tool.name == "web_fetch":
  â”‚   â”‚   â”‚       â””â”€ Calls: result = _execute_web_fetch(tool.input['url'])
  â”‚   â”‚   â”‚
  â”‚   â”‚   â”œâ”€ Builds: tool_results = [{type: "tool_result", tool_use_id, content}]
  â”‚   â”‚   â”‚
  â”‚   â”‚   â””â”€ Calls: response = self.client.messages.create(
  â”‚   â”‚              model="claude-sonnet-4-5-20250929",
  â”‚   â”‚              max_tokens=8192,
  â”‚   â”‚              messages=[...previous_messages, tool_results]
  â”‚   â”‚            )
  â”‚   â”‚
  â”‚   â””â”€ BREAK when stop_reason == "end_turn"
  â”‚
  â”œâ”€ Extracts: final_text = response.content[-1].text
  â”œâ”€ Parses: analysis_data = json.loads(final_text)
  â””â”€ Returns: {
               allergens_detected: [...],
               pfas_detected: [...],
               other_concerns: [...],
               confidence: 0.0-1.0
             }

ðŸ“„ claude_agent.py::analyze_extracted_product(
      product_data: Dict,
      url: str
    ) â†’ Dict
  â”‚
  â”œâ”€ Similar to analyze_product but:
  â”‚   â”œâ”€ Prompt includes pre-extracted product_data
  â”‚   â”œâ”€ Only web_search tool (no web_fetch needed)
  â”‚   â””â”€ Claude searches for manufacturer safety data, MSDS sheets, reviews
  â”‚
  â””â”€ Returns: analysis_data

ðŸ“„ claude_agent.py::_execute_web_search(query: str) â†’ str
  â”œâ”€ Reads: settings.serper_api_key
  â”œâ”€ Calls: response = httpx.post(
  â”‚          'https://google.serper.dev/search',
  â”‚          json={'q': query, 'num': 10},
  â”‚          headers={'X-API-KEY': api_key},
  â”‚          timeout=10
  â”‚        )
  â”œâ”€ Parses: results = response.json()
  â””â”€ Returns: json.dumps(results)

ðŸ“„ claude_agent.py::_execute_web_fetch(url: str) â†’ str
  â”œâ”€ Calls: response = httpx.get(url, timeout=10, headers={'User-Agent': '...'})
  â”œâ”€ Returns: response.text (on success)
  â””â”€ Returns: f"Error: {str(e)}" (on failure)

ðŸ“„ claude_agent.py::find_alternatives(...) â†’ List[Dict]
  â””â”€ Returns: [] (TODO: Phase 4 implementation stub)
```

### Claude Query Service (Data Extraction - No Tools)

```
ðŸ“„ claude_query.py::ClaudeQueryService.__init__()
  â”œâ”€ Reads: settings.anthropic_api_key
  â””â”€ Creates: self.client = Anthropic(api_key=...)

ðŸ“„ claude_query.py::extract_product_data(html: str) â†’ Dict
  â”‚
  â”œâ”€ Builds: prompt = "Extract structured product data from HTML:\n" + html
  â”‚
  â”œâ”€ Calls: response = self.client.messages.create(
  â”‚          model="claude-sonnet-4-5-20250929",
  â”‚          max_tokens=4096,
  â”‚          messages=[{role: "user", content: prompt}]
  â”‚        )
  â”‚   (NO TOOLS - pure text extraction)
  â”‚
  â”œâ”€ Extracts: text = response.content[0].text
  â”œâ”€ Parses: data = json.loads(text)
  â””â”€ Returns: {
               product_name: str,
               brand: str,
               ingredients: List[str],
               materials: List[str],
               features: List[str],
               warnings: List[str]
             }

ðŸ“„ claude_query.py::extract_review_insights(html: str) â†’ ReviewInsights
  â”‚
  â”œâ”€ Similar to extract_product_data
  â”œâ”€ Prompt: "Extract safety-related insights from product reviews..."
  â”‚
  â””â”€ Returns: ReviewInsights(
               common_concerns: List[str],
               positive_safety_notes: List[str],
               recurring_issues: List[str],
               confidence: float
             )
```

### Product Scraper Service (Orchestration)

```
ðŸ“„ product_scraper.py::ProductScraperService.__init__()
  â””â”€ (No initialization needed)

ðŸ“„ product_scraper.py::try_scrape(url: str) â†’ ScrapedProduct | None
  â”‚
  â”œâ”€ Calls: scraper = ScraperFactory.get_scraper(url)
  â”‚
  â”œâ”€ IF scraper exists:
  â”‚   â”œâ”€ Calls: result = await scraper.scrape(url)
  â”‚   â””â”€ Returns: result (ScrapedProduct)
  â”‚
  â””â”€ ELSE:
      â””â”€ Returns: None
```

---

## File-Level Import Relationships

```
config.py
  IMPORTS:
    - pydantic_settings.{BaseSettings, SettingsConfigDict}
  IMPORTED BY:
    - ../../run.py
    - ../api/main.py
    - ../api/auth.py
    - ./claude_agent.py
    - ./claude_query.py
    - ./database.py

database.py
  IMPORTS:
    - supabase.{create_client, Client}
    - .config.settings
  IMPORTED BY:
    - ../api/routes/analyze.py

claude_agent.py
  IMPORTS:
    - anthropic.{Anthropic, RateLimitError, APIError}
    - httpx (for web_search and web_fetch tools)
    - .config.settings
  IMPORTED BY:
    - ../api/routes/analyze.py

claude_query.py
  IMPORTS:
    - anthropic.Anthropic
    - .config.settings
    - ..domain.models.{ScrapedProduct, ReviewInsights}
  IMPORTED BY:
    - ../api/routes/analyze.py

product_scraper.py
  IMPORTS:
    - .scrapers.factory.ScraperFactory
    - ..domain.models.ScrapedProduct
  IMPORTED BY:
    - ../api/routes/analyze.py
```

---

## Directory Structure

```
/backend/src/infrastructure/
â”œâ”€â”€ __init__.py              # Package marker (empty)
â”œâ”€â”€ config.py                # Environment configuration (Pydantic settings)
â”œâ”€â”€ database.py              # Supabase client and queries
â”œâ”€â”€ claude_agent.py          # Claude AI with tools (web_search, web_fetch)
â”œâ”€â”€ claude_query.py          # Claude AI for data extraction (no tools)
â”œâ”€â”€ product_scraper.py       # Scraping orchestration service
â””â”€â”€ scrapers/                # Web scraper implementations â†’ [scrapers/CLAUDE.md](./scrapers/CLAUDE.md)
    â”œâ”€â”€ __init__.py          # Package marker (empty)
    â”œâ”€â”€ base.py              # Abstract base scraper
    â”œâ”€â”€ factory.py           # Scraper factory (URL-based selection)
    â””â”€â”€ amazon.py            # Amazon product page scraper
```

---

## Files Description

### config.py
**Purpose**: Centralized configuration management using Pydantic settings

**Key Features**:
- Loads environment variables from `.env`
- Validates required API keys and URLs
- Type-safe configuration access

**Dependencies**: None (pure Pydantic)

**Relationships**:
- Imported by nearly all backend modules
- Single source of truth for configuration

### database.py
**Purpose**: Supabase PostgreSQL database client and operations

**Key Functions**:
- `generate_url_hash()` - SHA256 hash for cache keys
- `get_cached_analysis()` - Retrieve cached product analysis
- `store_analysis()` - Store new analysis results

**Dependencies**:
- Supabase Python client
- config.settings

**Relationships**:
- Called by `api/routes/analyze.py` for caching
- Implements 30-day cache TTL (via Supabase RLS policies)

### claude_agent.py
**Purpose**: Claude AI agent with tool use for product safety analysis

**Key Features**:
- Uses Claude Sonnet 4.5 with tools
- web_search tool (via Serper API)
- web_fetch tool (direct HTTP requests)
- Agentic loop for multi-step analysis

**Key Functions**:
- `analyze_product()` - Full analysis with web scraping by Claude
- `analyze_extracted_product()` - Analysis of pre-scraped data
- `_execute_web_search()` - Google search via Serper
- `_execute_web_fetch()` - HTTP GET requests

**Dependencies**:
- Anthropic SDK
- httpx for HTTP requests
- config.settings

**Relationships**:
- Called by `api/routes/analyze.py`
- Main AI intelligence of the system

### claude_query.py
**Purpose**: Claude AI for structured data extraction (no tools)

**Key Features**:
- Pure text extraction from HTML
- No tool use (faster, cheaper)
- Structured JSON output

**Key Functions**:
- `extract_product_data()` - Extract product details from HTML
- `extract_review_insights()` - Extract safety insights from reviews

**Dependencies**:
- Anthropic SDK
- domain.models

**Relationships**:
- Called by `api/routes/analyze.py` after successful scraping
- Converts raw HTML to structured data

### product_scraper.py
**Purpose**: Orchestrates web scraping using appropriate scrapers

**Key Functions**:
- `try_scrape()` - Attempts to scrape product page

**Dependencies**:
- scrapers.factory
- domain.models

**Relationships**:
- Called by `api/routes/analyze.py`
- Delegates to appropriate scraper via factory pattern

---

## Architecture Patterns

### Separation of Concerns

**claude_agent.py vs claude_query.py**:
- `claude_agent.py` - Complex analysis with tools (agentic, multi-step)
- `claude_query.py` - Simple extraction without tools (fast, single-shot)

**Why separate?**
- Different use cases require different configurations
- Tool use adds latency and cost
- Extraction is deterministic, analysis is exploratory

### Factory Pattern

**Scraper selection**:
- Factory pattern in `scrapers/factory.py`
- URL-based scraper selection
- Easily extensible for new retailers

---

## Related Documentation

- [Backend Source](../CLAUDE.md) - Source overview
- [Scrapers](./scrapers/CLAUDE.md) - Web scraper implementations
- [API Routes](../api/routes/CLAUDE.md) - Where infrastructure services are called

---

Last Updated: 2025-11-18
